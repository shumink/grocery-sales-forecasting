{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fb95b0-76d4-4b08-93ab-d7db71db2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py7zr scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c196cbd0-2594-4cfb-b6fc-cf5c8dbc677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge lightgbm==4.1.0 -y --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c441c29b-a7c2-435e-8194-fbfeede3b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_WEEKS = 0\n",
    "TRAIN_WEEKS = 8\n",
    "MAX_MOVING_AVERAGE_WINDOW_WEEKS = 1\n",
    "MODEL_PATH = 'model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d16fb966-1052-4beb-90cd-f2d029044c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "from features import (\n",
    "    read_csv_from_7z,\n",
    "    StoreFeatureService,\n",
    "    ItemFeatureService,\n",
    "    DateFeatureService,\n",
    "    MovingAverageFeatureService\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af4c4b3-a6dc-4005-b5f7-62132e582ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric used by the competition\n",
    "class NWRMSLE:\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def NWRMSLE(self, a, p, w):\n",
    "        a = np.array(a)\n",
    "        p = np.array(p)\n",
    "        w = np.array(w)\n",
    "        \n",
    "        a = np.maximum(a, 0) + self.epsilon\n",
    "        p = np.maximum(p, 0) + self.epsilon\n",
    "        \n",
    "        weighted_errors = np.dot(np.square(p - a), np.transpose(w))\n",
    "        weights_sum = np.sum(w)\n",
    "        return math.sqrt(weighted_errors / weights_sum)\n",
    "\n",
    "    def NWRMSLE_lgb(self, preds, train_data):\n",
    "        labels = train_data.get_label()\n",
    "        w = train_data.get_weight()  \n",
    "        NWRMSLE_score = self.NWRMSLE(labels, preds, w)\n",
    "        return 'NWRMSLE', NWRMSLE_score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c685eb5-7e29-4820-9918-97e5247ac2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shumin/projects/grocery-sales-forecasting/features.py:23: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(io.BytesIO(data))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = read_csv_from_7z('dataset/train.csv.7z')\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "end_date = train_df['date'].max()\n",
    "start_date = end_date - timedelta(weeks=TRAIN_WEEKS+EVAL_WEEKS+MAX_MOVING_AVERAGE_WINDOW_WEEKS)\n",
    "train_df = train_df[train_df['date'] >= start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb17e48a-1347-4753-9368-b77ba55f8c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/1wcswnjs10954t19vpp13kgm0000gn/T/ipykernel_44500/1342417289.py:8: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df['onpromotion'] = train_df['onpromotion'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "target = 'unit_sales'\n",
    "\n",
    "# As we know from the EDA, the unit_sales column heavily skew to the left, so I decide \n",
    "# sense to apply log1p here to make it easier for the model to predict\n",
    "train_df[target] = train_df[target].clip(lower=0).apply(np.log1p)\n",
    "\n",
    "\n",
    "train_df['onpromotion'] = train_df['onpromotion'].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969b9e4-68c7-4d49-b1ff-546cf77e069e",
   "metadata": {},
   "source": [
    "# Fetching features from feature services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3214e3-e08f-4961-a5fd-d820d0c5c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fs = StoreFeatureService()\n",
    "item_fs = ItemFeatureService()\n",
    "date_fs = DateFeatureService()\n",
    "\n",
    "ma_s_i_7d_fs = MovingAverageFeatureService([\"store_nbr\", \"item_nbr\", \"date\"], 7)\n",
    "ma_i_7d_fs = MovingAverageFeatureService([\"item_nbr\", \"date\"], 7)\n",
    "ma_s_7d_fs = MovingAverageFeatureService([\"store_nbr\", \"date\"], 7)\n",
    "\n",
    "ma_s_i_7d_fs = MovingAverageFeatureService([\"store_nbr\", \"item_nbr\", \"date\"], 7)\n",
    "ma_i_7d_fs = MovingAverageFeatureService([\"item_nbr\", \"date\"], 7)\n",
    "ma_s_7d_fs = MovingAverageFeatureService([\"store_nbr\", \"date\"], 7)\n",
    "\n",
    "ma_s_i_3d_fs = MovingAverageFeatureService([\"store_nbr\", \"item_nbr\", \"date\"], 3)\n",
    "ma_i_3d_fs = MovingAverageFeatureService([\"item_nbr\", \"date\"], 3)\n",
    "ma_s_3d_fs = MovingAverageFeatureService([\"store_nbr\", \"date\"], 3)\n",
    "\n",
    "all_feature_services = [store_fs, item_fs, date_fs, ma_s_i_7d_fs, ma_i_7d_fs, ma_s_7d_fs, ma_s_i_3d_fs, ma_i_3d_fs, ma_s_3d_fs,]\n",
    "\n",
    "categorical = ['onpromotion']\n",
    "continuous = []\n",
    "\n",
    "for fs in all_feature_services:\n",
    "    categorical.extend(fs.categorical)\n",
    "    continuous.extend(fs.continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52551174-d14b-4349-a404-2cdb59aeec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shumin/projects/grocery-sales-forecasting/features.py:28: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  rolling_mean = df.groupby(group_cols).sum()['unit_sales'].rolling(window, min_periods=1).mean().unstack(id_cols).shift(1,freq=\"D\").stack(id_cols)\n",
      "/Users/shumin/projects/grocery-sales-forecasting/features.py:28: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  rolling_mean = df.groupby(group_cols).sum()['unit_sales'].rolling(window, min_periods=1).mean().unstack(id_cols).shift(1,freq=\"D\").stack(id_cols)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_df.copy()\n",
    "\n",
    "# Joining the features together\n",
    "for fs in all_feature_services:\n",
    "    train_set = fs.join(train_set)\n",
    "train_set['sample_weight'] = train_set['perishable'].map(lambda x: 1.0 if x==0 else 1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d488b-0662-4e7b-b126-436e5f07f35b",
   "metadata": {},
   "source": [
    "# Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681a3ec4-7a7f-42ee-a4aa-fb799f70e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an ordinal encoder to map distinct categories to numbers\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "features = categorical + continuous\n",
    "\n",
    "end_date = train_set['date'].max()\n",
    "\n",
    "if EVAL_WEEKS > 0:\n",
    "    eval_end_date = end_date\n",
    "    eval_start_date = eval_end_date - timedelta(weeks=EVAL_WEEKS)\n",
    "    train_end_date = eval_start_date\n",
    "    train_start_date = train_end_date - timedelta(weeks=TRAIN_WEEKS)\n",
    "else:\n",
    "    train_end_date = end_date\n",
    "    train_start_date = train_end_date - timedelta(weeks=TRAIN_WEEKS)\n",
    "\n",
    "\n",
    "train_data = train_set[(train_set['date'] >= train_start_date) & (train_set['date'] < train_end_date)]\n",
    "train_data = train_data.drop(columns=['date', 'id'])\n",
    "\n",
    "# Fit the encoder to training set\n",
    "train_data[categorical] = ordinal_encoder.fit_transform(train_data[categorical])\n",
    "\n",
    "\n",
    "# X_train = train_data[features]\n",
    "# y_train = train_data[target]\n",
    "# w_train = train_data['sample_weight']\n",
    "train_dataset = lgb.Dataset(train_data[features], label=train_data[target], weight=train_data['sample_weight'], categorical_feature=categorical)\n",
    "\n",
    "valid_sets = [train_dataset]\n",
    "if EVAL_WEEKS > 0:\n",
    "    eval_data = train_set[(train_set['date'] >= eval_start_date) & (train_set['date'] <= eval_end_date)]\n",
    "    eval_data = eval_data.drop(columns=['date', 'id'])\n",
    "    # Apply the same mapping to the eval set\n",
    "    eval_data[categorical] = ordinal_encoder.transform(eval_data[categorical])\n",
    "    # X_eval = eval_data[features]\n",
    "    # y_eval = eval_data[target]\n",
    "    # w_eval = eval_data['sample_weight']\n",
    "    eval_dataset = lgb.Dataset(eval_data[features], label=eval_data[target], weight=eval_data['sample_weight'], categorical_feature=categorical, reference=train_dataset)\n",
    "    valid_sets.append(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b6904e-57b8-4ab3-b94a-cd5f4ef65535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=1500, min_child_samples=10 will be ignored. Current value: min_data_in_leaf=1500\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1500, min_child_samples=10 will be ignored. Current value: min_data_in_leaf=1500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5490\n",
      "[LightGBM] [Info] Number of data points in the train set: 5892313, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 1.710765\n",
      "[1]\ttraining's l2: 0.748064\ttraining's NWRMSLE: 0.864907\n",
      "[2]\ttraining's l2: 0.730733\ttraining's NWRMSLE: 0.854829\n",
      "[3]\ttraining's l2: 0.713035\ttraining's NWRMSLE: 0.844414\n",
      "[4]\ttraining's l2: 0.696166\ttraining's NWRMSLE: 0.834366\n",
      "[5]\ttraining's l2: 0.67982\ttraining's NWRMSLE: 0.824512\n",
      "[6]\ttraining's l2: 0.664135\ttraining's NWRMSLE: 0.814945\n",
      "[7]\ttraining's l2: 0.649031\ttraining's NWRMSLE: 0.805625\n",
      "[8]\ttraining's l2: 0.634569\ttraining's NWRMSLE: 0.796598\n",
      "[9]\ttraining's l2: 0.620673\ttraining's NWRMSLE: 0.787828\n",
      "[10]\ttraining's l2: 0.607277\ttraining's NWRMSLE: 0.77928\n",
      "[11]\ttraining's l2: 0.594412\ttraining's NWRMSLE: 0.770981\n",
      "[12]\ttraining's l2: 0.582052\ttraining's NWRMSLE: 0.762923\n",
      "[13]\ttraining's l2: 0.570163\ttraining's NWRMSLE: 0.755091\n",
      "[14]\ttraining's l2: 0.558793\ttraining's NWRMSLE: 0.747524\n",
      "[15]\ttraining's l2: 0.54841\ttraining's NWRMSLE: 0.740547\n",
      "[16]\ttraining's l2: 0.537827\ttraining's NWRMSLE: 0.733367\n",
      "[17]\ttraining's l2: 0.527641\ttraining's NWRMSLE: 0.726389\n",
      "[18]\ttraining's l2: 0.518398\ttraining's NWRMSLE: 0.719999\n",
      "[19]\ttraining's l2: 0.508983\ttraining's NWRMSLE: 0.71343\n",
      "[20]\ttraining's l2: 0.499933\ttraining's NWRMSLE: 0.707059\n",
      "[21]\ttraining's l2: 0.491258\ttraining's NWRMSLE: 0.700898\n",
      "[22]\ttraining's l2: 0.482869\ttraining's NWRMSLE: 0.694888\n",
      "[23]\ttraining's l2: 0.474834\ttraining's NWRMSLE: 0.689082\n",
      "[24]\ttraining's l2: 0.467084\ttraining's NWRMSLE: 0.683435\n",
      "[25]\ttraining's l2: 0.460047\ttraining's NWRMSLE: 0.678268\n",
      "[26]\ttraining's l2: 0.453297\ttraining's NWRMSLE: 0.673273\n",
      "[27]\ttraining's l2: 0.446388\ttraining's NWRMSLE: 0.668122\n",
      "[28]\ttraining's l2: 0.439699\ttraining's NWRMSLE: 0.663098\n",
      "[29]\ttraining's l2: 0.433357\ttraining's NWRMSLE: 0.658299\n",
      "[30]\ttraining's l2: 0.427199\ttraining's NWRMSLE: 0.653605\n",
      "[31]\ttraining's l2: 0.42129\ttraining's NWRMSLE: 0.649069\n",
      "[32]\ttraining's l2: 0.415571\ttraining's NWRMSLE: 0.644648\n",
      "[33]\ttraining's l2: 0.410483\ttraining's NWRMSLE: 0.640689\n",
      "[34]\ttraining's l2: 0.405218\ttraining's NWRMSLE: 0.636567\n",
      "[35]\ttraining's l2: 0.400153\ttraining's NWRMSLE: 0.632576\n",
      "[36]\ttraining's l2: 0.396845\ttraining's NWRMSLE: 0.629956\n",
      "[37]\ttraining's l2: 0.393638\ttraining's NWRMSLE: 0.627405\n",
      "[38]\ttraining's l2: 0.389014\ttraining's NWRMSLE: 0.623709\n",
      "[39]\ttraining's l2: 0.384783\ttraining's NWRMSLE: 0.620309\n",
      "[40]\ttraining's l2: 0.380703\ttraining's NWRMSLE: 0.617011\n",
      "[41]\ttraining's l2: 0.376517\ttraining's NWRMSLE: 0.61361\n",
      "[42]\ttraining's l2: 0.372524\ttraining's NWRMSLE: 0.610347\n",
      "[43]\ttraining's l2: 0.368641\ttraining's NWRMSLE: 0.607158\n",
      "[44]\ttraining's l2: 0.364934\ttraining's NWRMSLE: 0.604097\n",
      "[45]\ttraining's l2: 0.361579\ttraining's NWRMSLE: 0.601314\n",
      "[46]\ttraining's l2: 0.359246\ttraining's NWRMSLE: 0.599372\n",
      "[47]\ttraining's l2: 0.355901\ttraining's NWRMSLE: 0.596574\n",
      "[48]\ttraining's l2: 0.352621\ttraining's NWRMSLE: 0.593819\n",
      "[49]\ttraining's l2: 0.349502\ttraining's NWRMSLE: 0.591187\n",
      "[50]\ttraining's l2: 0.346708\ttraining's NWRMSLE: 0.588819\n",
      "[51]\ttraining's l2: 0.34381\ttraining's NWRMSLE: 0.586353\n",
      "[52]\ttraining's l2: 0.341256\ttraining's NWRMSLE: 0.584171\n",
      "[53]\ttraining's l2: 0.338548\ttraining's NWRMSLE: 0.581848\n",
      "[54]\ttraining's l2: 0.335941\ttraining's NWRMSLE: 0.579605\n",
      "[55]\ttraining's l2: 0.333465\ttraining's NWRMSLE: 0.577464\n",
      "[56]\ttraining's l2: 0.331198\ttraining's NWRMSLE: 0.575498\n",
      "[57]\ttraining's l2: 0.328881\ttraining's NWRMSLE: 0.573481\n",
      "[58]\ttraining's l2: 0.326621\ttraining's NWRMSLE: 0.571507\n",
      "[59]\ttraining's l2: 0.324447\ttraining's NWRMSLE: 0.569602\n",
      "[60]\ttraining's l2: 0.322358\ttraining's NWRMSLE: 0.567766\n",
      "[61]\ttraining's l2: 0.320357\ttraining's NWRMSLE: 0.566\n",
      "[62]\ttraining's l2: 0.318409\ttraining's NWRMSLE: 0.564277\n",
      "[63]\ttraining's l2: 0.316728\ttraining's NWRMSLE: 0.562786\n",
      "[64]\ttraining's l2: 0.314946\ttraining's NWRMSLE: 0.561201\n",
      "[65]\ttraining's l2: 0.313247\ttraining's NWRMSLE: 0.559684\n",
      "[66]\ttraining's l2: 0.31158\ttraining's NWRMSLE: 0.558193\n",
      "[67]\ttraining's l2: 0.309977\ttraining's NWRMSLE: 0.556756\n",
      "[68]\ttraining's l2: 0.308416\ttraining's NWRMSLE: 0.555352\n",
      "[69]\ttraining's l2: 0.307028\ttraining's NWRMSLE: 0.554101\n",
      "[70]\ttraining's l2: 0.30596\ttraining's NWRMSLE: 0.553136\n",
      "[71]\ttraining's l2: 0.304541\ttraining's NWRMSLE: 0.551852\n",
      "[72]\ttraining's l2: 0.303302\ttraining's NWRMSLE: 0.550729\n",
      "[73]\ttraining's l2: 0.301972\ttraining's NWRMSLE: 0.54952\n",
      "[74]\ttraining's l2: 0.300703\ttraining's NWRMSLE: 0.548364\n",
      "[75]\ttraining's l2: 0.299442\ttraining's NWRMSLE: 0.547213\n",
      "[76]\ttraining's l2: 0.298262\ttraining's NWRMSLE: 0.546134\n",
      "[77]\ttraining's l2: 0.297148\ttraining's NWRMSLE: 0.545113\n",
      "[78]\ttraining's l2: 0.296034\ttraining's NWRMSLE: 0.54409\n",
      "[79]\ttraining's l2: 0.295039\ttraining's NWRMSLE: 0.543175\n",
      "[80]\ttraining's l2: 0.293988\ttraining's NWRMSLE: 0.542207\n",
      "[81]\ttraining's l2: 0.29309\ttraining's NWRMSLE: 0.541378\n",
      "[82]\ttraining's l2: 0.292345\ttraining's NWRMSLE: 0.54069\n",
      "[83]\ttraining's l2: 0.291414\ttraining's NWRMSLE: 0.539828\n",
      "[84]\ttraining's l2: 0.290485\ttraining's NWRMSLE: 0.538967\n",
      "[85]\ttraining's l2: 0.289603\ttraining's NWRMSLE: 0.538148\n",
      "[86]\ttraining's l2: 0.288743\ttraining's NWRMSLE: 0.537348\n",
      "[87]\ttraining's l2: 0.287924\ttraining's NWRMSLE: 0.536585\n",
      "[88]\ttraining's l2: 0.287147\ttraining's NWRMSLE: 0.535861\n",
      "[89]\ttraining's l2: 0.286549\ttraining's NWRMSLE: 0.535302\n",
      "[90]\ttraining's l2: 0.285783\ttraining's NWRMSLE: 0.534587\n",
      "[91]\ttraining's l2: 0.285027\ttraining's NWRMSLE: 0.533879\n",
      "[92]\ttraining's l2: 0.284318\ttraining's NWRMSLE: 0.533215\n",
      "[93]\ttraining's l2: 0.283642\ttraining's NWRMSLE: 0.53258\n",
      "[94]\ttraining's l2: 0.282994\ttraining's NWRMSLE: 0.531972\n",
      "[95]\ttraining's l2: 0.282365\ttraining's NWRMSLE: 0.53138\n",
      "[96]\ttraining's l2: 0.281781\ttraining's NWRMSLE: 0.530831\n",
      "[97]\ttraining's l2: 0.281348\ttraining's NWRMSLE: 0.530422\n",
      "[98]\ttraining's l2: 0.280728\ttraining's NWRMSLE: 0.529838\n",
      "[99]\ttraining's l2: 0.280302\ttraining's NWRMSLE: 0.529435\n",
      "[100]\ttraining's l2: 0.279741\ttraining's NWRMSLE: 0.528905\n",
      "[101]\ttraining's l2: 0.279206\ttraining's NWRMSLE: 0.528399\n",
      "[102]\ttraining's l2: 0.278752\ttraining's NWRMSLE: 0.52797\n",
      "[103]\ttraining's l2: 0.278259\ttraining's NWRMSLE: 0.527502\n",
      "[104]\ttraining's l2: 0.277786\ttraining's NWRMSLE: 0.527054\n",
      "[105]\ttraining's l2: 0.277313\ttraining's NWRMSLE: 0.526605\n",
      "[106]\ttraining's l2: 0.276852\ttraining's NWRMSLE: 0.526167\n",
      "[107]\ttraining's l2: 0.276451\ttraining's NWRMSLE: 0.525786\n",
      "[108]\ttraining's l2: 0.276006\ttraining's NWRMSLE: 0.525362\n",
      "[109]\ttraining's l2: 0.275585\ttraining's NWRMSLE: 0.524962\n",
      "[110]\ttraining's l2: 0.275195\ttraining's NWRMSLE: 0.52459\n",
      "[111]\ttraining's l2: 0.274817\ttraining's NWRMSLE: 0.52423\n",
      "[112]\ttraining's l2: 0.274448\ttraining's NWRMSLE: 0.523877\n",
      "[113]\ttraining's l2: 0.274133\ttraining's NWRMSLE: 0.523578\n",
      "[114]\ttraining's l2: 0.273762\ttraining's NWRMSLE: 0.523222\n",
      "[115]\ttraining's l2: 0.273488\ttraining's NWRMSLE: 0.522961\n",
      "[116]\ttraining's l2: 0.273159\ttraining's NWRMSLE: 0.522646\n",
      "[117]\ttraining's l2: 0.272815\ttraining's NWRMSLE: 0.522317\n",
      "[118]\ttraining's l2: 0.27249\ttraining's NWRMSLE: 0.522005\n",
      "[119]\ttraining's l2: 0.272186\ttraining's NWRMSLE: 0.521715\n",
      "[120]\ttraining's l2: 0.271893\ttraining's NWRMSLE: 0.521434\n",
      "[121]\ttraining's l2: 0.271604\ttraining's NWRMSLE: 0.521156\n",
      "[122]\ttraining's l2: 0.271318\ttraining's NWRMSLE: 0.520882\n",
      "[123]\ttraining's l2: 0.271056\ttraining's NWRMSLE: 0.520631\n",
      "[124]\ttraining's l2: 0.270788\ttraining's NWRMSLE: 0.520373\n",
      "[125]\ttraining's l2: 0.270543\ttraining's NWRMSLE: 0.520138\n",
      "[126]\ttraining's l2: 0.270271\ttraining's NWRMSLE: 0.519876\n",
      "[127]\ttraining's l2: 0.270015\ttraining's NWRMSLE: 0.519629\n",
      "[128]\ttraining's l2: 0.269779\ttraining's NWRMSLE: 0.519402\n",
      "[129]\ttraining's l2: 0.269512\ttraining's NWRMSLE: 0.519145\n",
      "[130]\ttraining's l2: 0.26924\ttraining's NWRMSLE: 0.518883\n",
      "[131]\ttraining's l2: 0.269015\ttraining's NWRMSLE: 0.518667\n",
      "[132]\ttraining's l2: 0.268793\ttraining's NWRMSLE: 0.518452\n",
      "[133]\ttraining's l2: 0.268573\ttraining's NWRMSLE: 0.518241\n",
      "[134]\ttraining's l2: 0.268351\ttraining's NWRMSLE: 0.518026\n",
      "[135]\ttraining's l2: 0.268152\ttraining's NWRMSLE: 0.517834\n",
      "[136]\ttraining's l2: 0.267951\ttraining's NWRMSLE: 0.51764\n",
      "[137]\ttraining's l2: 0.267765\ttraining's NWRMSLE: 0.51746\n",
      "[138]\ttraining's l2: 0.267574\ttraining's NWRMSLE: 0.517276\n",
      "[139]\ttraining's l2: 0.267392\ttraining's NWRMSLE: 0.517099\n",
      "[140]\ttraining's l2: 0.267194\ttraining's NWRMSLE: 0.516908\n",
      "[141]\ttraining's l2: 0.267033\ttraining's NWRMSLE: 0.516753\n",
      "[142]\ttraining's l2: 0.26683\ttraining's NWRMSLE: 0.516556\n",
      "[143]\ttraining's l2: 0.266652\ttraining's NWRMSLE: 0.516384\n",
      "[144]\ttraining's l2: 0.266504\ttraining's NWRMSLE: 0.51624\n",
      "[145]\ttraining's l2: 0.266323\ttraining's NWRMSLE: 0.516065\n",
      "[146]\ttraining's l2: 0.266126\ttraining's NWRMSLE: 0.515874\n",
      "[147]\ttraining's l2: 0.265964\ttraining's NWRMSLE: 0.515717\n",
      "[148]\ttraining's l2: 0.265772\ttraining's NWRMSLE: 0.51553\n",
      "[149]\ttraining's l2: 0.265601\ttraining's NWRMSLE: 0.515365\n",
      "[150]\ttraining's l2: 0.265414\ttraining's NWRMSLE: 0.515184\n",
      "[151]\ttraining's l2: 0.265268\ttraining's NWRMSLE: 0.515042\n",
      "[152]\ttraining's l2: 0.265096\ttraining's NWRMSLE: 0.514874\n",
      "[153]\ttraining's l2: 0.264957\ttraining's NWRMSLE: 0.514739\n",
      "[154]\ttraining's l2: 0.264816\ttraining's NWRMSLE: 0.514603\n",
      "[155]\ttraining's l2: 0.264683\ttraining's NWRMSLE: 0.514473\n",
      "[156]\ttraining's l2: 0.264537\ttraining's NWRMSLE: 0.514332\n",
      "[157]\ttraining's l2: 0.264411\ttraining's NWRMSLE: 0.51421\n",
      "[158]\ttraining's l2: 0.264289\ttraining's NWRMSLE: 0.51409\n",
      "[159]\ttraining's l2: 0.264155\ttraining's NWRMSLE: 0.51396\n",
      "[160]\ttraining's l2: 0.26403\ttraining's NWRMSLE: 0.513839\n",
      "[161]\ttraining's l2: 0.263917\ttraining's NWRMSLE: 0.513729\n",
      "[162]\ttraining's l2: 0.26377\ttraining's NWRMSLE: 0.513585\n",
      "[163]\ttraining's l2: 0.263652\ttraining's NWRMSLE: 0.51347\n",
      "[164]\ttraining's l2: 0.263509\ttraining's NWRMSLE: 0.513331\n",
      "[165]\ttraining's l2: 0.263401\ttraining's NWRMSLE: 0.513226\n",
      "[166]\ttraining's l2: 0.263288\ttraining's NWRMSLE: 0.513116\n",
      "[167]\ttraining's l2: 0.263171\ttraining's NWRMSLE: 0.513002\n",
      "[168]\ttraining's l2: 0.263078\ttraining's NWRMSLE: 0.512911\n",
      "[169]\ttraining's l2: 0.262964\ttraining's NWRMSLE: 0.5128\n",
      "[170]\ttraining's l2: 0.262888\ttraining's NWRMSLE: 0.512726\n",
      "[171]\ttraining's l2: 0.262783\ttraining's NWRMSLE: 0.512623\n",
      "[172]\ttraining's l2: 0.262648\ttraining's NWRMSLE: 0.512492\n",
      "[173]\ttraining's l2: 0.262538\ttraining's NWRMSLE: 0.512385\n",
      "[174]\ttraining's l2: 0.262422\ttraining's NWRMSLE: 0.512271\n",
      "[175]\ttraining's l2: 0.262305\ttraining's NWRMSLE: 0.512158\n",
      "[176]\ttraining's l2: 0.262219\ttraining's NWRMSLE: 0.512073\n",
      "[177]\ttraining's l2: 0.26213\ttraining's NWRMSLE: 0.511986\n",
      "[178]\ttraining's l2: 0.261984\ttraining's NWRMSLE: 0.511844\n",
      "[179]\ttraining's l2: 0.26183\ttraining's NWRMSLE: 0.511693\n",
      "[180]\ttraining's l2: 0.261722\ttraining's NWRMSLE: 0.511588\n",
      "[181]\ttraining's l2: 0.261641\ttraining's NWRMSLE: 0.511508\n",
      "[182]\ttraining's l2: 0.261524\ttraining's NWRMSLE: 0.511394\n",
      "[183]\ttraining's l2: 0.261445\ttraining's NWRMSLE: 0.511317\n",
      "[184]\ttraining's l2: 0.261361\ttraining's NWRMSLE: 0.511234\n",
      "[185]\ttraining's l2: 0.261252\ttraining's NWRMSLE: 0.511128\n",
      "[186]\ttraining's l2: 0.261169\ttraining's NWRMSLE: 0.511047\n",
      "[187]\ttraining's l2: 0.261042\ttraining's NWRMSLE: 0.510922\n",
      "[188]\ttraining's l2: 0.260954\ttraining's NWRMSLE: 0.510837\n",
      "[189]\ttraining's l2: 0.26087\ttraining's NWRMSLE: 0.510754\n",
      "[190]\ttraining's l2: 0.260775\ttraining's NWRMSLE: 0.510661\n",
      "[191]\ttraining's l2: 0.260658\ttraining's NWRMSLE: 0.510547\n",
      "[192]\ttraining's l2: 0.260544\ttraining's NWRMSLE: 0.510435\n",
      "[193]\ttraining's l2: 0.260459\ttraining's NWRMSLE: 0.510352\n",
      "[194]\ttraining's l2: 0.26039\ttraining's NWRMSLE: 0.510284\n",
      "[195]\ttraining's l2: 0.260309\ttraining's NWRMSLE: 0.510205\n",
      "[196]\ttraining's l2: 0.260187\ttraining's NWRMSLE: 0.510085\n",
      "[197]\ttraining's l2: 0.260081\ttraining's NWRMSLE: 0.509981\n",
      "[198]\ttraining's l2: 0.259992\ttraining's NWRMSLE: 0.509894\n",
      "[199]\ttraining's l2: 0.259881\ttraining's NWRMSLE: 0.509785\n",
      "[200]\ttraining's l2: 0.259783\ttraining's NWRMSLE: 0.509689\n",
      "[201]\ttraining's l2: 0.259722\ttraining's NWRMSLE: 0.509629\n",
      "[202]\ttraining's l2: 0.259655\ttraining's NWRMSLE: 0.509564\n",
      "[203]\ttraining's l2: 0.259552\ttraining's NWRMSLE: 0.509462\n",
      "[204]\ttraining's l2: 0.259467\ttraining's NWRMSLE: 0.509379\n",
      "[205]\ttraining's l2: 0.25938\ttraining's NWRMSLE: 0.509294\n",
      "[206]\ttraining's l2: 0.259301\ttraining's NWRMSLE: 0.509216\n",
      "[207]\ttraining's l2: 0.259232\ttraining's NWRMSLE: 0.509149\n",
      "[208]\ttraining's l2: 0.259164\ttraining's NWRMSLE: 0.509081\n",
      "[209]\ttraining's l2: 0.25908\ttraining's NWRMSLE: 0.508999\n",
      "[210]\ttraining's l2: 0.259006\ttraining's NWRMSLE: 0.508926\n",
      "[211]\ttraining's l2: 0.258905\ttraining's NWRMSLE: 0.508827\n",
      "[212]\ttraining's l2: 0.258854\ttraining's NWRMSLE: 0.508777\n",
      "[213]\ttraining's l2: 0.258785\ttraining's NWRMSLE: 0.508709\n",
      "[214]\ttraining's l2: 0.258736\ttraining's NWRMSLE: 0.508661\n",
      "[215]\ttraining's l2: 0.25866\ttraining's NWRMSLE: 0.508586\n",
      "[216]\ttraining's l2: 0.258567\ttraining's NWRMSLE: 0.508495\n",
      "[217]\ttraining's l2: 0.258509\ttraining's NWRMSLE: 0.508438\n",
      "[218]\ttraining's l2: 0.258419\ttraining's NWRMSLE: 0.50835\n",
      "[219]\ttraining's l2: 0.258361\ttraining's NWRMSLE: 0.508292\n",
      "[220]\ttraining's l2: 0.258305\ttraining's NWRMSLE: 0.508237\n",
      "[221]\ttraining's l2: 0.258202\ttraining's NWRMSLE: 0.508136\n",
      "[222]\ttraining's l2: 0.258136\ttraining's NWRMSLE: 0.508071\n",
      "[223]\ttraining's l2: 0.258058\ttraining's NWRMSLE: 0.507994\n",
      "[224]\ttraining's l2: 0.257989\ttraining's NWRMSLE: 0.507926\n",
      "[225]\ttraining's l2: 0.257908\ttraining's NWRMSLE: 0.507846\n",
      "[226]\ttraining's l2: 0.257845\ttraining's NWRMSLE: 0.507784\n",
      "[227]\ttraining's l2: 0.257766\ttraining's NWRMSLE: 0.507707\n",
      "[228]\ttraining's l2: 0.257671\ttraining's NWRMSLE: 0.507613\n",
      "[229]\ttraining's l2: 0.257617\ttraining's NWRMSLE: 0.50756\n",
      "[230]\ttraining's l2: 0.257542\ttraining's NWRMSLE: 0.507486\n",
      "[231]\ttraining's l2: 0.25747\ttraining's NWRMSLE: 0.507415\n",
      "[232]\ttraining's l2: 0.257382\ttraining's NWRMSLE: 0.507328\n",
      "[233]\ttraining's l2: 0.257306\ttraining's NWRMSLE: 0.507253\n",
      "[234]\ttraining's l2: 0.257244\ttraining's NWRMSLE: 0.507192\n",
      "[235]\ttraining's l2: 0.257174\ttraining's NWRMSLE: 0.507123\n",
      "[236]\ttraining's l2: 0.257131\ttraining's NWRMSLE: 0.507081\n",
      "[237]\ttraining's l2: 0.257061\ttraining's NWRMSLE: 0.507012\n",
      "[238]\ttraining's l2: 0.25701\ttraining's NWRMSLE: 0.506961\n",
      "[239]\ttraining's l2: 0.256937\ttraining's NWRMSLE: 0.50689\n",
      "[240]\ttraining's l2: 0.256888\ttraining's NWRMSLE: 0.506841\n",
      "[241]\ttraining's l2: 0.256823\ttraining's NWRMSLE: 0.506777\n",
      "[242]\ttraining's l2: 0.256768\ttraining's NWRMSLE: 0.506723\n",
      "[243]\ttraining's l2: 0.256702\ttraining's NWRMSLE: 0.506657\n",
      "[244]\ttraining's l2: 0.256651\ttraining's NWRMSLE: 0.506608\n",
      "[245]\ttraining's l2: 0.256584\ttraining's NWRMSLE: 0.506541\n",
      "[246]\ttraining's l2: 0.256534\ttraining's NWRMSLE: 0.506492\n",
      "[247]\ttraining's l2: 0.256474\ttraining's NWRMSLE: 0.506432\n",
      "[248]\ttraining's l2: 0.256398\ttraining's NWRMSLE: 0.506358\n",
      "[249]\ttraining's l2: 0.256339\ttraining's NWRMSLE: 0.506299\n",
      "[250]\ttraining's l2: 0.256282\ttraining's NWRMSLE: 0.506243\n",
      "[251]\ttraining's l2: 0.256232\ttraining's NWRMSLE: 0.506193\n",
      "[252]\ttraining's l2: 0.256197\ttraining's NWRMSLE: 0.506159\n",
      "[253]\ttraining's l2: 0.256137\ttraining's NWRMSLE: 0.5061\n",
      "[254]\ttraining's l2: 0.256082\ttraining's NWRMSLE: 0.506045\n",
      "[255]\ttraining's l2: 0.256038\ttraining's NWRMSLE: 0.506002\n",
      "[256]\ttraining's l2: 0.255978\ttraining's NWRMSLE: 0.505943\n",
      "[257]\ttraining's l2: 0.255937\ttraining's NWRMSLE: 0.505902\n",
      "[258]\ttraining's l2: 0.255894\ttraining's NWRMSLE: 0.50586\n",
      "[259]\ttraining's l2: 0.255826\ttraining's NWRMSLE: 0.505793\n",
      "[260]\ttraining's l2: 0.255796\ttraining's NWRMSLE: 0.505763\n",
      "[261]\ttraining's l2: 0.255755\ttraining's NWRMSLE: 0.505722\n",
      "[262]\ttraining's l2: 0.255697\ttraining's NWRMSLE: 0.505665\n",
      "[263]\ttraining's l2: 0.255631\ttraining's NWRMSLE: 0.505599\n",
      "[264]\ttraining's l2: 0.255586\ttraining's NWRMSLE: 0.505555\n",
      "[265]\ttraining's l2: 0.255541\ttraining's NWRMSLE: 0.505511\n",
      "[266]\ttraining's l2: 0.255501\ttraining's NWRMSLE: 0.505471\n",
      "[267]\ttraining's l2: 0.255469\ttraining's NWRMSLE: 0.505439\n",
      "[268]\ttraining's l2: 0.255438\ttraining's NWRMSLE: 0.505409\n",
      "[269]\ttraining's l2: 0.255399\ttraining's NWRMSLE: 0.50537\n",
      "[270]\ttraining's l2: 0.255344\ttraining's NWRMSLE: 0.505315\n",
      "[271]\ttraining's l2: 0.255296\ttraining's NWRMSLE: 0.505268\n",
      "[272]\ttraining's l2: 0.255244\ttraining's NWRMSLE: 0.505217\n",
      "[273]\ttraining's l2: 0.255217\ttraining's NWRMSLE: 0.50519\n",
      "[274]\ttraining's l2: 0.255176\ttraining's NWRMSLE: 0.50515\n",
      "[275]\ttraining's l2: 0.255136\ttraining's NWRMSLE: 0.50511\n",
      "[276]\ttraining's l2: 0.255077\ttraining's NWRMSLE: 0.505051\n",
      "[277]\ttraining's l2: 0.255026\ttraining's NWRMSLE: 0.505001\n",
      "[278]\ttraining's l2: 0.254978\ttraining's NWRMSLE: 0.504953\n",
      "[279]\ttraining's l2: 0.254948\ttraining's NWRMSLE: 0.504924\n",
      "[280]\ttraining's l2: 0.254917\ttraining's NWRMSLE: 0.504893\n",
      "[281]\ttraining's l2: 0.254872\ttraining's NWRMSLE: 0.504849\n",
      "[282]\ttraining's l2: 0.25484\ttraining's NWRMSLE: 0.504816\n",
      "[283]\ttraining's l2: 0.254795\ttraining's NWRMSLE: 0.504772\n",
      "[284]\ttraining's l2: 0.254758\ttraining's NWRMSLE: 0.504736\n",
      "[285]\ttraining's l2: 0.254722\ttraining's NWRMSLE: 0.5047\n",
      "[286]\ttraining's l2: 0.25466\ttraining's NWRMSLE: 0.504639\n",
      "[287]\ttraining's l2: 0.254619\ttraining's NWRMSLE: 0.504598\n",
      "[288]\ttraining's l2: 0.254579\ttraining's NWRMSLE: 0.504558\n",
      "[289]\ttraining's l2: 0.254531\ttraining's NWRMSLE: 0.50451\n",
      "[290]\ttraining's l2: 0.254483\ttraining's NWRMSLE: 0.504463\n",
      "[291]\ttraining's l2: 0.25444\ttraining's NWRMSLE: 0.50442\n",
      "[292]\ttraining's l2: 0.254397\ttraining's NWRMSLE: 0.504377\n",
      "[293]\ttraining's l2: 0.254339\ttraining's NWRMSLE: 0.504321\n",
      "[294]\ttraining's l2: 0.254298\ttraining's NWRMSLE: 0.504279\n",
      "[295]\ttraining's l2: 0.254252\ttraining's NWRMSLE: 0.504235\n",
      "[296]\ttraining's l2: 0.254199\ttraining's NWRMSLE: 0.504181\n",
      "[297]\ttraining's l2: 0.25415\ttraining's NWRMSLE: 0.504133\n",
      "[298]\ttraining's l2: 0.254105\ttraining's NWRMSLE: 0.504088\n",
      "[299]\ttraining's l2: 0.254063\ttraining's NWRMSLE: 0.504047\n",
      "[300]\ttraining's l2: 0.254023\ttraining's NWRMSLE: 0.504007\n",
      "[301]\ttraining's l2: 0.25398\ttraining's NWRMSLE: 0.503964\n",
      "[302]\ttraining's l2: 0.25392\ttraining's NWRMSLE: 0.503905\n",
      "[303]\ttraining's l2: 0.253883\ttraining's NWRMSLE: 0.503868\n",
      "[304]\ttraining's l2: 0.253839\ttraining's NWRMSLE: 0.503825\n",
      "[305]\ttraining's l2: 0.253788\ttraining's NWRMSLE: 0.503774\n",
      "[306]\ttraining's l2: 0.253744\ttraining's NWRMSLE: 0.50373\n",
      "[307]\ttraining's l2: 0.253695\ttraining's NWRMSLE: 0.503681\n",
      "[308]\ttraining's l2: 0.253656\ttraining's NWRMSLE: 0.503642\n",
      "[309]\ttraining's l2: 0.25362\ttraining's NWRMSLE: 0.503607\n",
      "[310]\ttraining's l2: 0.25358\ttraining's NWRMSLE: 0.503567\n",
      "[311]\ttraining's l2: 0.253551\ttraining's NWRMSLE: 0.503538\n",
      "[312]\ttraining's l2: 0.253497\ttraining's NWRMSLE: 0.503485\n",
      "[313]\ttraining's l2: 0.253448\ttraining's NWRMSLE: 0.503436\n",
      "[314]\ttraining's l2: 0.25341\ttraining's NWRMSLE: 0.503398\n",
      "[315]\ttraining's l2: 0.253381\ttraining's NWRMSLE: 0.503369\n",
      "[316]\ttraining's l2: 0.253341\ttraining's NWRMSLE: 0.50333\n",
      "[317]\ttraining's l2: 0.253292\ttraining's NWRMSLE: 0.503281\n",
      "[318]\ttraining's l2: 0.253265\ttraining's NWRMSLE: 0.503254\n",
      "[319]\ttraining's l2: 0.253213\ttraining's NWRMSLE: 0.503203\n",
      "[320]\ttraining's l2: 0.253179\ttraining's NWRMSLE: 0.503169\n",
      "[321]\ttraining's l2: 0.253144\ttraining's NWRMSLE: 0.503135\n",
      "[322]\ttraining's l2: 0.253109\ttraining's NWRMSLE: 0.503099\n",
      "[323]\ttraining's l2: 0.253074\ttraining's NWRMSLE: 0.503065\n",
      "[324]\ttraining's l2: 0.25303\ttraining's NWRMSLE: 0.503021\n",
      "[325]\ttraining's l2: 0.252985\ttraining's NWRMSLE: 0.502976\n",
      "[326]\ttraining's l2: 0.252959\ttraining's NWRMSLE: 0.50295\n",
      "[327]\ttraining's l2: 0.25292\ttraining's NWRMSLE: 0.502911\n",
      "[328]\ttraining's l2: 0.25288\ttraining's NWRMSLE: 0.502872\n",
      "[329]\ttraining's l2: 0.252847\ttraining's NWRMSLE: 0.502839\n",
      "[330]\ttraining's l2: 0.252812\ttraining's NWRMSLE: 0.502804\n",
      "[331]\ttraining's l2: 0.252774\ttraining's NWRMSLE: 0.502766\n",
      "[332]\ttraining's l2: 0.25274\ttraining's NWRMSLE: 0.502732\n",
      "[333]\ttraining's l2: 0.252708\ttraining's NWRMSLE: 0.502701\n",
      "[334]\ttraining's l2: 0.252664\ttraining's NWRMSLE: 0.502656\n",
      "[335]\ttraining's l2: 0.252624\ttraining's NWRMSLE: 0.502618\n",
      "[336]\ttraining's l2: 0.25259\ttraining's NWRMSLE: 0.502583\n",
      "[337]\ttraining's l2: 0.252553\ttraining's NWRMSLE: 0.502546\n",
      "[338]\ttraining's l2: 0.252515\ttraining's NWRMSLE: 0.502509\n",
      "[339]\ttraining's l2: 0.252484\ttraining's NWRMSLE: 0.502477\n",
      "[340]\ttraining's l2: 0.252455\ttraining's NWRMSLE: 0.502449\n",
      "[341]\ttraining's l2: 0.252427\ttraining's NWRMSLE: 0.502421\n",
      "[342]\ttraining's l2: 0.252389\ttraining's NWRMSLE: 0.502384\n",
      "[343]\ttraining's l2: 0.252362\ttraining's NWRMSLE: 0.502356\n",
      "[344]\ttraining's l2: 0.252323\ttraining's NWRMSLE: 0.502318\n",
      "[345]\ttraining's l2: 0.252289\ttraining's NWRMSLE: 0.502284\n",
      "[346]\ttraining's l2: 0.252253\ttraining's NWRMSLE: 0.502248\n",
      "[347]\ttraining's l2: 0.252226\ttraining's NWRMSLE: 0.502221\n",
      "[348]\ttraining's l2: 0.252188\ttraining's NWRMSLE: 0.502183\n",
      "[349]\ttraining's l2: 0.252155\ttraining's NWRMSLE: 0.50215\n",
      "[350]\ttraining's l2: 0.252114\ttraining's NWRMSLE: 0.502109\n",
      "[351]\ttraining's l2: 0.252082\ttraining's NWRMSLE: 0.502078\n",
      "[352]\ttraining's l2: 0.252051\ttraining's NWRMSLE: 0.502047\n",
      "[353]\ttraining's l2: 0.25203\ttraining's NWRMSLE: 0.502026\n",
      "[354]\ttraining's l2: 0.251987\ttraining's NWRMSLE: 0.501983\n",
      "[355]\ttraining's l2: 0.251955\ttraining's NWRMSLE: 0.501951\n",
      "[356]\ttraining's l2: 0.251924\ttraining's NWRMSLE: 0.501921\n",
      "[357]\ttraining's l2: 0.251888\ttraining's NWRMSLE: 0.501885\n",
      "[358]\ttraining's l2: 0.251862\ttraining's NWRMSLE: 0.501858\n",
      "[359]\ttraining's l2: 0.251828\ttraining's NWRMSLE: 0.501825\n",
      "[360]\ttraining's l2: 0.251799\ttraining's NWRMSLE: 0.501796\n",
      "[361]\ttraining's l2: 0.25177\ttraining's NWRMSLE: 0.501767\n",
      "[362]\ttraining's l2: 0.251744\ttraining's NWRMSLE: 0.501741\n",
      "[363]\ttraining's l2: 0.251717\ttraining's NWRMSLE: 0.501714\n",
      "[364]\ttraining's l2: 0.251685\ttraining's NWRMSLE: 0.501683\n",
      "[365]\ttraining's l2: 0.251651\ttraining's NWRMSLE: 0.501649\n",
      "[366]\ttraining's l2: 0.25162\ttraining's NWRMSLE: 0.501617\n",
      "[367]\ttraining's l2: 0.25159\ttraining's NWRMSLE: 0.501587\n",
      "[368]\ttraining's l2: 0.25157\ttraining's NWRMSLE: 0.501567\n",
      "[369]\ttraining's l2: 0.251537\ttraining's NWRMSLE: 0.501534\n",
      "[370]\ttraining's l2: 0.251506\ttraining's NWRMSLE: 0.501504\n",
      "[371]\ttraining's l2: 0.251477\ttraining's NWRMSLE: 0.501475\n",
      "[372]\ttraining's l2: 0.251447\ttraining's NWRMSLE: 0.501445\n",
      "[373]\ttraining's l2: 0.251417\ttraining's NWRMSLE: 0.501415\n",
      "[374]\ttraining's l2: 0.251381\ttraining's NWRMSLE: 0.501379\n",
      "[375]\ttraining's l2: 0.251361\ttraining's NWRMSLE: 0.501359\n",
      "[376]\ttraining's l2: 0.251332\ttraining's NWRMSLE: 0.50133\n",
      "[377]\ttraining's l2: 0.251304\ttraining's NWRMSLE: 0.501302\n",
      "[378]\ttraining's l2: 0.251274\ttraining's NWRMSLE: 0.501272\n",
      "[379]\ttraining's l2: 0.251243\ttraining's NWRMSLE: 0.501241\n",
      "[380]\ttraining's l2: 0.251216\ttraining's NWRMSLE: 0.501215\n",
      "[381]\ttraining's l2: 0.251187\ttraining's NWRMSLE: 0.501186\n",
      "[382]\ttraining's l2: 0.251163\ttraining's NWRMSLE: 0.501162\n",
      "[383]\ttraining's l2: 0.25114\ttraining's NWRMSLE: 0.501139\n",
      "[384]\ttraining's l2: 0.251117\ttraining's NWRMSLE: 0.501116\n",
      "[385]\ttraining's l2: 0.251099\ttraining's NWRMSLE: 0.501098\n",
      "[386]\ttraining's l2: 0.251066\ttraining's NWRMSLE: 0.501064\n",
      "[387]\ttraining's l2: 0.251038\ttraining's NWRMSLE: 0.501037\n",
      "[388]\ttraining's l2: 0.251018\ttraining's NWRMSLE: 0.501017\n",
      "[389]\ttraining's l2: 0.250991\ttraining's NWRMSLE: 0.50099\n",
      "[390]\ttraining's l2: 0.250971\ttraining's NWRMSLE: 0.50097\n",
      "[391]\ttraining's l2: 0.250941\ttraining's NWRMSLE: 0.50094\n",
      "[392]\ttraining's l2: 0.250913\ttraining's NWRMSLE: 0.500912\n",
      "[393]\ttraining's l2: 0.250886\ttraining's NWRMSLE: 0.500885\n",
      "[394]\ttraining's l2: 0.250848\ttraining's NWRMSLE: 0.500847\n",
      "[395]\ttraining's l2: 0.250825\ttraining's NWRMSLE: 0.500825\n",
      "[396]\ttraining's l2: 0.250799\ttraining's NWRMSLE: 0.500798\n",
      "[397]\ttraining's l2: 0.250773\ttraining's NWRMSLE: 0.500772\n",
      "[398]\ttraining's l2: 0.250738\ttraining's NWRMSLE: 0.500737\n",
      "[399]\ttraining's l2: 0.250717\ttraining's NWRMSLE: 0.500716\n",
      "[400]\ttraining's l2: 0.250675\ttraining's NWRMSLE: 0.500675\n",
      "[401]\ttraining's l2: 0.250651\ttraining's NWRMSLE: 0.50065\n",
      "[402]\ttraining's l2: 0.250625\ttraining's NWRMSLE: 0.500625\n",
      "[403]\ttraining's l2: 0.250599\ttraining's NWRMSLE: 0.500598\n",
      "[404]\ttraining's l2: 0.25058\ttraining's NWRMSLE: 0.500579\n",
      "[405]\ttraining's l2: 0.250559\ttraining's NWRMSLE: 0.500559\n",
      "[406]\ttraining's l2: 0.250534\ttraining's NWRMSLE: 0.500533\n",
      "[407]\ttraining's l2: 0.250506\ttraining's NWRMSLE: 0.500506\n",
      "[408]\ttraining's l2: 0.250477\ttraining's NWRMSLE: 0.500477\n",
      "[409]\ttraining's l2: 0.250458\ttraining's NWRMSLE: 0.500458\n",
      "[410]\ttraining's l2: 0.250436\ttraining's NWRMSLE: 0.500436\n",
      "[411]\ttraining's l2: 0.250418\ttraining's NWRMSLE: 0.500418\n",
      "[412]\ttraining's l2: 0.250387\ttraining's NWRMSLE: 0.500386\n",
      "[413]\ttraining's l2: 0.250365\ttraining's NWRMSLE: 0.500365\n",
      "[414]\ttraining's l2: 0.250337\ttraining's NWRMSLE: 0.500337\n",
      "[415]\ttraining's l2: 0.250303\ttraining's NWRMSLE: 0.500303\n",
      "[416]\ttraining's l2: 0.250278\ttraining's NWRMSLE: 0.500278\n",
      "[417]\ttraining's l2: 0.250248\ttraining's NWRMSLE: 0.500248\n",
      "[418]\ttraining's l2: 0.25023\ttraining's NWRMSLE: 0.50023\n",
      "[419]\ttraining's l2: 0.250205\ttraining's NWRMSLE: 0.500205\n",
      "[420]\ttraining's l2: 0.250181\ttraining's NWRMSLE: 0.500181\n",
      "[421]\ttraining's l2: 0.250157\ttraining's NWRMSLE: 0.500157\n",
      "[422]\ttraining's l2: 0.250137\ttraining's NWRMSLE: 0.500137\n",
      "[423]\ttraining's l2: 0.250115\ttraining's NWRMSLE: 0.500115\n",
      "[424]\ttraining's l2: 0.250088\ttraining's NWRMSLE: 0.500088\n",
      "[425]\ttraining's l2: 0.250071\ttraining's NWRMSLE: 0.500071\n",
      "[426]\ttraining's l2: 0.250051\ttraining's NWRMSLE: 0.500051\n",
      "[427]\ttraining's l2: 0.250022\ttraining's NWRMSLE: 0.500022\n",
      "[428]\ttraining's l2: 0.249999\ttraining's NWRMSLE: 0.499999\n",
      "[429]\ttraining's l2: 0.249974\ttraining's NWRMSLE: 0.499974\n",
      "[430]\ttraining's l2: 0.249955\ttraining's NWRMSLE: 0.499955\n",
      "[431]\ttraining's l2: 0.249931\ttraining's NWRMSLE: 0.499931\n",
      "[432]\ttraining's l2: 0.24991\ttraining's NWRMSLE: 0.49991\n",
      "[433]\ttraining's l2: 0.249886\ttraining's NWRMSLE: 0.499886\n",
      "[434]\ttraining's l2: 0.249872\ttraining's NWRMSLE: 0.499872\n",
      "[435]\ttraining's l2: 0.24985\ttraining's NWRMSLE: 0.49985\n",
      "[436]\ttraining's l2: 0.249831\ttraining's NWRMSLE: 0.499831\n",
      "[437]\ttraining's l2: 0.249807\ttraining's NWRMSLE: 0.499807\n",
      "[438]\ttraining's l2: 0.249783\ttraining's NWRMSLE: 0.499783\n",
      "[439]\ttraining's l2: 0.249762\ttraining's NWRMSLE: 0.499762\n",
      "[440]\ttraining's l2: 0.249744\ttraining's NWRMSLE: 0.499744\n",
      "[441]\ttraining's l2: 0.249723\ttraining's NWRMSLE: 0.499723\n",
      "[442]\ttraining's l2: 0.249703\ttraining's NWRMSLE: 0.499702\n",
      "[443]\ttraining's l2: 0.249686\ttraining's NWRMSLE: 0.499686\n",
      "[444]\ttraining's l2: 0.249648\ttraining's NWRMSLE: 0.499648\n",
      "[445]\ttraining's l2: 0.249629\ttraining's NWRMSLE: 0.499629\n",
      "[446]\ttraining's l2: 0.249611\ttraining's NWRMSLE: 0.499611\n",
      "[447]\ttraining's l2: 0.249584\ttraining's NWRMSLE: 0.499584\n",
      "[448]\ttraining's l2: 0.249559\ttraining's NWRMSLE: 0.499559\n",
      "[449]\ttraining's l2: 0.249515\ttraining's NWRMSLE: 0.499515\n",
      "[450]\ttraining's l2: 0.24949\ttraining's NWRMSLE: 0.49949\n",
      "[451]\ttraining's l2: 0.249468\ttraining's NWRMSLE: 0.499468\n",
      "[452]\ttraining's l2: 0.249448\ttraining's NWRMSLE: 0.499448\n",
      "[453]\ttraining's l2: 0.249423\ttraining's NWRMSLE: 0.499422\n",
      "[454]\ttraining's l2: 0.249393\ttraining's NWRMSLE: 0.499392\n",
      "[455]\ttraining's l2: 0.249372\ttraining's NWRMSLE: 0.499372\n",
      "[456]\ttraining's l2: 0.249356\ttraining's NWRMSLE: 0.499355\n",
      "[457]\ttraining's l2: 0.249338\ttraining's NWRMSLE: 0.499337\n",
      "[458]\ttraining's l2: 0.249309\ttraining's NWRMSLE: 0.499308\n",
      "[459]\ttraining's l2: 0.24929\ttraining's NWRMSLE: 0.499289\n",
      "[460]\ttraining's l2: 0.24927\ttraining's NWRMSLE: 0.49927\n",
      "[461]\ttraining's l2: 0.24925\ttraining's NWRMSLE: 0.499249\n",
      "[462]\ttraining's l2: 0.249223\ttraining's NWRMSLE: 0.499223\n",
      "[463]\ttraining's l2: 0.249203\ttraining's NWRMSLE: 0.499202\n",
      "[464]\ttraining's l2: 0.249184\ttraining's NWRMSLE: 0.499183\n",
      "[465]\ttraining's l2: 0.249171\ttraining's NWRMSLE: 0.49917\n",
      "[466]\ttraining's l2: 0.249143\ttraining's NWRMSLE: 0.499143\n",
      "[467]\ttraining's l2: 0.249113\ttraining's NWRMSLE: 0.499112\n",
      "[468]\ttraining's l2: 0.249094\ttraining's NWRMSLE: 0.499093\n",
      "[469]\ttraining's l2: 0.249078\ttraining's NWRMSLE: 0.499077\n",
      "[470]\ttraining's l2: 0.249061\ttraining's NWRMSLE: 0.49906\n",
      "[471]\ttraining's l2: 0.249039\ttraining's NWRMSLE: 0.499038\n",
      "[472]\ttraining's l2: 0.249013\ttraining's NWRMSLE: 0.499012\n",
      "[473]\ttraining's l2: 0.24899\ttraining's NWRMSLE: 0.498989\n",
      "[474]\ttraining's l2: 0.24897\ttraining's NWRMSLE: 0.498969\n",
      "[475]\ttraining's l2: 0.248951\ttraining's NWRMSLE: 0.49895\n",
      "[476]\ttraining's l2: 0.248919\ttraining's NWRMSLE: 0.498917\n",
      "[477]\ttraining's l2: 0.248902\ttraining's NWRMSLE: 0.498901\n",
      "[478]\ttraining's l2: 0.248856\ttraining's NWRMSLE: 0.498855\n",
      "[479]\ttraining's l2: 0.248831\ttraining's NWRMSLE: 0.49883\n",
      "[480]\ttraining's l2: 0.248817\ttraining's NWRMSLE: 0.498816\n",
      "[481]\ttraining's l2: 0.248783\ttraining's NWRMSLE: 0.498782\n",
      "[482]\ttraining's l2: 0.24874\ttraining's NWRMSLE: 0.498739\n",
      "[483]\ttraining's l2: 0.248723\ttraining's NWRMSLE: 0.498721\n",
      "[484]\ttraining's l2: 0.248706\ttraining's NWRMSLE: 0.498705\n",
      "[485]\ttraining's l2: 0.248683\ttraining's NWRMSLE: 0.498681\n",
      "[486]\ttraining's l2: 0.248666\ttraining's NWRMSLE: 0.498664\n",
      "[487]\ttraining's l2: 0.248649\ttraining's NWRMSLE: 0.498647\n",
      "[488]\ttraining's l2: 0.248627\ttraining's NWRMSLE: 0.498625\n",
      "[489]\ttraining's l2: 0.24861\ttraining's NWRMSLE: 0.498608\n",
      "[490]\ttraining's l2: 0.248586\ttraining's NWRMSLE: 0.498584\n",
      "[491]\ttraining's l2: 0.248569\ttraining's NWRMSLE: 0.498567\n",
      "[492]\ttraining's l2: 0.248549\ttraining's NWRMSLE: 0.498547\n",
      "[493]\ttraining's l2: 0.248529\ttraining's NWRMSLE: 0.498526\n",
      "[494]\ttraining's l2: 0.248512\ttraining's NWRMSLE: 0.49851\n",
      "[495]\ttraining's l2: 0.248496\ttraining's NWRMSLE: 0.498493\n",
      "[496]\ttraining's l2: 0.248477\ttraining's NWRMSLE: 0.498475\n",
      "[497]\ttraining's l2: 0.248453\ttraining's NWRMSLE: 0.49845\n",
      "[498]\ttraining's l2: 0.248436\ttraining's NWRMSLE: 0.498434\n",
      "[499]\ttraining's l2: 0.24842\ttraining's NWRMSLE: 0.498417\n",
      "[500]\ttraining's l2: 0.248401\ttraining's NWRMSLE: 0.498399\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'num_leaves': 33,\n",
    "    'objective': 'regression',\n",
    "    'min_data_in_leaf': 1500,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.7,\n",
    "    'min_split_gain': 0,\n",
    "    'metric': 'l2',\n",
    "    'subsample': 0.9,\n",
    "    'drop_rate': 0.1,\n",
    "    'min_child_samples': 10,\n",
    "    'min_child_weight': 150,\n",
    "    'max_drop': 50,\n",
    "    'boosting':'gbdt',\n",
    "    'num_threads':6,\n",
    "}\n",
    "\n",
    "\n",
    "nwrmsle = NWRMSLE()\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_dataset,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=valid_sets,\n",
    "    callbacks=[lgb.log_evaluation()],\n",
    "    feval=nwrmsle.NWRMSLE_lgb,\n",
    ")\n",
    "# Mark the completion time\n",
    "completion_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef18553-ab45-4764-98c8-a2cb52c3d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save model to two locations, one tagged with the completion time, the other being the latest.\n",
    "# This is assuming that we never use old data to train a prod model.\n",
    "# The following data is saved:\n",
    "# Training metadata - this is for monitoring\n",
    "# Encoder - this is to keep feature encoding the same\n",
    "# Model artifact\n",
    "\n",
    "for model_version in [completion_time.strftime(\"%Y-%m-%d_%H-%M-%S\"), 'latest']:\n",
    "    model_push_path = os.path.join(MODEL_PATH, model_version)\n",
    "    pathlib.Path(model_push_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    metadata = {\n",
    "        \"completion_time\": completion_time.strftime('%s'),\n",
    "        \"training_set_end_date\": str(train_set.date.max()),\n",
    "        \"training_set_start_date\": str(train_set.date.min()),\n",
    "        \"categorical_features\": categorical,\n",
    "        \"continuous_features\": continuous\n",
    "    }\n",
    "\n",
    "    \n",
    "    with open(os.path.join(model_push_path,'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata,f)\n",
    "\n",
    "    with open(os.path.join(model_push_path,'encoder'), 'wb') as f:\n",
    "        pickle.dump(ordinal_encoder, f)\n",
    "\n",
    "    model.save_model(os.path.join(model_push_path,'model.txt'), num_iteration=model.best_iteration) \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece9801-0ebe-4eff-b61f-89456b75ad4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
